##ğŸ“„ End-to-End RAG Application using LangChain & Gemini

This project demonstrates a complete Retrieval-Augmented Generation (RAG) pipeline using LangChain, Google Gemini (Generative AI), ChromaDB, and PDF documents.
It enables users to ask natural-language questions and receive accurate, source-grounded answers extracted from documents.

##ğŸš€ What is RAG (Retrieval-Augmented Generation)?

Retrieval-Augmented Generation (RAG) is an AI architecture that combines:

Information Retrieval (searching relevant documents)

Generative AI (LLMs generating human-like answers)

Instead of relying only on the modelâ€™s training data, RAG retrieves relevant context from external data sources (PDFs, databases, web pages) and feeds that context to the LLM before generating a response.

##ğŸ§  Why RAG is Important

Traditional LLMs:

Can hallucinate

Donâ€™t know private or updated data

Canâ€™t cite sources

RAG solves this by:

Grounding responses in real documents

Improving accuracy and trust

Allowing domain-specific knowledge (PDFs, internal docs)

Returning source documents along with answers

##ğŸ› ï¸ Tech Stack Used

Google Gemini (Generative AI) â€“ LLM for answer generation

LangChain â€“ Orchestration framework

ChromaDB â€“ Vector database for semantic search

PyPDF â€“ PDF document loading

Recursive Text Splitter â€“ Chunking documents

Google Generative AI Embeddings â€“ Text embeddings

Python / Google Colab

##ğŸ“‚ Project Workflow (RAG Pipeline)
PDF Document
   â†“
Document Loader (PyPDFLoader)
   â†“
Text Chunking (RecursiveCharacterTextSplitter)
   â†“
Embeddings (GoogleGenerativeAIEmbeddings)
   â†“
Vector Store (ChromaDB)
   â†“
Retriever
   â†“
Gemini LLM
   â†“
Final Answer + Source Documents

##ğŸ§© Step-by-Step Code Explanation
####1ï¸âƒ£ Install Dependencies
!pip install -q --upgrade google-generativeai langchain-google-genai
!pip install langchain_community chromadb pypdf

####2ï¸âƒ£ Load API Key Securely (Google Colab)
from google.colab import userdata
API_KEY = userdata.get('GOOGLE_API_KEY')

####3ï¸âƒ£ Initialize Gemini LLM
from langchain_google_genai import ChatGoogleGenerativeAI

model = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    api_key=API_KEY
)

####4ï¸âƒ£ Load PDF Documents
from langchain_community.document_loaders import PyPDFLoader

pdfloader = PyPDFLoader('/content/ai.pdf')
loaded_pdf_doc = pdfloader.load()

####5ï¸âƒ£ Split Documents into Chunks
from langchain.text_splitter import RecursiveCharacterTextSplitter

recursive_splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=50,
    separators=['\n\n', '\n', ' ']
)

chunks = recursive_splitter.split_documents(loaded_pdf_doc)

####6ï¸âƒ£ Generate Embeddings
from langchain_google_genai import GoogleGenerativeAIEmbeddings

embeddings = GoogleGenerativeAIEmbeddings(
    model='models/embedding-001',
    google_api_key=API_KEY
)

####7ï¸âƒ£ Store Embeddings in ChromaDB
from langchain.vectorstores import Chroma

vector_store = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings
)

####8ï¸âƒ£ Create Retriever
vector_index = vector_store.as_retriever(
    search_kwargs={'k': 8}
)

####9ï¸âƒ£ Build RAG Chain
from langchain.chains import RetrievalQA

rag = RetrievalQA.from_chain_type(
    llm=model,
    retriever=vector_index,
    return_source_documents=True
)

####ğŸ”Ÿ Ask Questions
question = "what is travel AI expert system according to the doc?"
response = rag({"query": question})

print(response)

##âœ… Output Example

Answer generated by Gemini

Source documents retrieved from the PDF

Page-level traceability for trust & verification

##ğŸ“Œ Use Cases of This RAG System

PDF Question Answering

Enterprise Knowledge Assistants

Research Paper Analysis

Legal / Medical / Financial Document QA

Internal Company Chatbots

Education & Learning Platforms

##ğŸ” Security Best Practices

API keys are never hardcoded

Keys are stored in Colab secrets / environment variables

.env / secrets.toml recommended for production

##ğŸ§  Key Takeaways

RAG combines search + generation

Improves factual correctness

Reduces hallucinations

Enables private & domain-specific AI systems

LangChain simplifies orchestration

##ğŸ“– Future Enhancements

Streamlit UI

Chat history memory

Multi-PDF support

Hybrid search (BM25 + Vector)

Re-ranking models

Deployment using Docker
